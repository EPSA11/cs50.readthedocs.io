{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic Regression Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EPSA11/cs50.readthedocs.io/blob/main/Logistic_Regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression Assignment\n",
        "## Implementing logistic regression from scratch in pytorch"
      ],
      "metadata": {
        "id": "eoEmXl9Mld38"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHz-2d7PcM1c"
      },
      "outputs": [],
      "source": [
        "# make the necessary imports\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will be training our logistic regression model on a diabetes dataset. "
      ],
      "metadata": {
        "id": "3k6xSBVTm04Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the give dataset on colab or if you are using your local machine, change the path to the folder in which the dataset is placed."
      ],
      "metadata": {
        "id": "kBp1geTllyBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset using pandas\n",
        "diabetes = pd.read_csv('diabetes.csv')"
      ],
      "metadata": {
        "id": "r3IO9c4ychQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pima Indians Diabetes Database has been gathered by the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of this dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage. This dataset contains the following features:\n",
        "\n",
        "Pregnancies\n",
        "- Glucose\n",
        "- BloodPressure\n",
        "- SkinThickness\n",
        "- Insuline\n",
        "- BMI\n",
        "- DiabetesPedigreeFunction\n",
        "- Age\n",
        "- Outcome (has diabetes or not)\n",
        "\n"
      ],
      "metadata": {
        "id": "oerR8VS4nip_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to see the content of the dataset, print the first few rows of the dataset,\n",
        "diabetes.head()"
      ],
      "metadata": {
        "id": "RUfdrlk2cvyS",
        "outputId": "7f6ad609-de0e-44e0-9e8d-f4a99869fe19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9d5ec6997747>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# to see the content of the dataset, print the first few rows of the dataset,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdiabetes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'diabetes' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# printing few stats for each feature we have in our dataset\n",
        "diabetes.describe()"
      ],
      "metadata": {
        "id": "RVfk1LdMcwLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing"
      ],
      "metadata": {
        "id": "TF4Z1yg8mfAf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert our pandas dataframe values to numpy arrays\n",
        "x_np = diabetes.iloc[:, :-1].values\n",
        "\n",
        "# Standardize the data\n",
        "x_norm = StandardScaler().fit_transform(x_np)\n",
        "\n",
        "# Convert from numpy to torch tensors\n",
        "### start code here ### \n",
        "X = \n",
        "### end code here ###\n",
        "\n",
        "X = X.float()"
      ],
      "metadata": {
        "id": "RMHaRiAwdES_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the last columns of the dataset to numpy, this is the variable to be predicted\n",
        "y_np = diabetes.iloc[:, -1].values\n",
        "\n",
        "# Convert from numpy to torch tensors\n",
        "### start code here ### \n",
        "y =  \n",
        "### end code here ###\n",
        "\n",
        "y = y.float().unsqueeze(1)"
      ],
      "metadata": {
        "id": "hDeC2v5EcyRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split our data into training and testing. We will only train our model on training data and get metrics from testing data"
      ],
      "metadata": {
        "id": "7_t5x46ao13H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.1)"
      ],
      "metadata": {
        "id": "MnxKFeIVeAdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model building"
      ],
      "metadata": {
        "id": "mP9TMConpByG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Logistic Regression\n",
        "The prediction of a logistic model is as follow:\n",
        "$$\n",
        "    \\large \\hat{y} = \\sigma(\\boldsymbol{X}\\boldsymbol{w})\n",
        "$$\n",
        "Where $\\sigma$ is the sigmoid or logit function:\n",
        "$$\n",
        "    \\large \\sigma(\\boldsymbol{x}) = \\frac{1}{1 + \\exp(-x)}\n",
        "$$<br>\n",
        "The prediction $\\hat{y}$ is obtained by matrix multiplication between the input $\\boldsymbol{X}$ and the weights of the model $\\boldsymbol{w}$ given as input to the logit function.<br/>\n",
        "The sigmoid function is used here because it squashes the input in the $[0, 1]$ range suitable for describing a Bernouilli distribution.<br>\n",
        "Where the Bernouilli cases are:\n",
        "- the patient has diabetes with $p$ probability\n",
        "- the patient does not have diabetes with $1 - p$ probability<br>\n",
        "\n",
        "It is important to note that the bias is included in $\\boldsymbol{X}$ as a column of ones.<br>\n",
        "\n",
        "Training a classification model can be expressed as maximizing the likelihood of the observed data.<br>\n",
        "In other words, we want the predicted probability of our model that a patient has diabetes to be as close as the true probability of the data.<br>\n",
        "In practice, maximizing the likelihood is equivalent to minimize the negative log likelihood:<br>\n",
        "$$\n",
        "    \\large L(\\boldsymbol{\\theta}) = - \\frac{1}{N}\\sum_{i=1}^{n}\\boldsymbol{y_i}\\log(\\hat{\\boldsymbol{y}}_i)\n",
        "$$<br>\n",
        "Because we dealing with a binary target, it is appropriate to use the binary cross entropy:<br>\n",
        "$$\n",
        "    \\large L(\\boldsymbol{\\theta}) = - \\frac{1}{N}\\sum_{i=1}^{n}\\boldsymbol{y_i}\\log(\\hat{\\boldsymbol{y}}_i) + (1 - \\boldsymbol{y_i})\\log(1 - \\hat{\\boldsymbol{y}}_i)\n",
        "$$"
      ],
      "metadata": {
        "id": "_f6yZP2vpqaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intialising weights and biases\n",
        "\n",
        "### start code here ###\n",
        "w =\n",
        "b =\n",
        "### end code here ###"
      ],
      "metadata": {
        "id": "Lb9Pt1LOgDhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid function that squashes the input between 0 and 1\"\"\"\n",
        "    \n",
        "    ### start code here ###\n",
        "    activation =\n",
        "    ### end code here ###\n",
        "\n",
        "    return activation"
      ],
      "metadata": {
        "id": "VA0y_KbpgHF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model(x):\n",
        "    \"\"\"Pedicts the class given the data and the weights\n",
        "    \n",
        "    Args:\n",
        "        x: A torch tensor for the input data.\n",
        "    \"\"\"\n",
        "    ### start code here ###\n",
        "    # perform linear regression x * w^T + b\n",
        "    linear_ =\n",
        "\n",
        "    # bound the output with sigmoid activation\n",
        "    linear_activation = \n",
        "    ### end code here ###\n",
        "    \n",
        "    return linear_activation"
      ],
      "metadata": {
        "id": "dt4zNUhpeEhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bce_loss(y_true, y_pred):\n",
        "    \"\"\"binary_cross_entropy\n",
        "    Loss function for the training of the logistic regression\n",
        "    \n",
        "    We add an epsilon inside the log functions to avoid Nans.\n",
        "    \n",
        "    Args:\n",
        "        y_true: A torch tensor for the labels of the data.\n",
        "        y_pred: A torch tensor for the values predicted by the model.\n",
        "    \"\"\"\n",
        "    ### start code here ###\n",
        "    # calculate lambda or 1/N in above figure\n",
        "    lambda_ = \n",
        "\n",
        "    # epsilon is added to contents in log function, this is to prevent finding log of zero as sigmoid is always greater or equal to zero\n",
        "    epsilon = 1e-10\n",
        "\n",
        "    # calculate\n",
        "    l_theta = \n",
        "    ### end code here ###\n",
        "\n",
        "    return l_theta"
      ],
      "metadata": {
        "id": "uDCXTEwAeayv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if your implementation is correct\n",
        "\n",
        "preds = model(X_train)\n",
        "loss = bce_loss(y_train, preds)\n",
        "print(f\"initial loss {loss.item()}\")"
      ],
      "metadata": {
        "id": "a45DNeMfhGnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adjust weights and biases using gradient descent\n",
        "\n",
        "We'll reduce the loss and improve our model using the gradient descent algorithm, which has the following steps:\n",
        "\n",
        "1. Generate predictions\n",
        "2. Calculate the loss\n",
        "3. Compute gradients w.r.t the weights and biases\n",
        "4. Adjust the weights by subtracting a small quantity proportional to the gradient\n",
        "5. Reset the gradients to zero"
      ],
      "metadata": {
        "id": "bEI8M9vIrusY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adjust weights & reset gradients\n",
        "# Train for _ epochs\n",
        "\n",
        "for i in range(_):\n",
        "\n",
        "    ### start code here ###\n",
        "    # generate predictions\n",
        "    preds =\n",
        "\n",
        "    # calculate loss\n",
        "    loss =\n",
        "\n",
        "    # calculate gradient w.r.t loss (backprop)\n",
        "    \n",
        "\n",
        "    # learning rate\n",
        "    learning_rate = \n",
        "    with torch.no_grad():\n",
        "\n",
        "        # update weights\n",
        "        w =  \n",
        "        b =  \n",
        "        \n",
        "        # zero out gradients\n",
        "         \n",
        "         \n",
        "    ### end code here ###\n",
        "    if i%100 == 0:\n",
        "      print(f\"epoch {i} loss {loss.item()}\")"
      ],
      "metadata": {
        "id": "T0n-GGs2gAKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model metrics on test data"
      ],
      "metadata": {
        "id": "bXx-NmelsVIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Find loss on test data\n",
        "y_pred = model(X_test)\n",
        "bn_test = bce_loss(y_test, y_pred).item()\n",
        "print('Binary cross-entropy on the test set:', bn_test)"
      ],
      "metadata": {
        "id": "IedjP8roev_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get accuracy, precision, recall and f1 score on test data"
      ],
      "metadata": {
        "id": "9Xrz7lEsskqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_binary_pred(y_true, y_pred):\n",
        "    \"\"\"Finds the best threshold based on the prediction and the labels\n",
        "    \n",
        "    Args:\n",
        "        y_true: A torch tensor for the labels of the data.\n",
        "        y_pred: A torch tensor for the values predicted by the model.\n",
        "    \"\"\"\n",
        "    y_pred_thr = y_pred.clone()\n",
        "    \n",
        "    accs = []\n",
        "    thrs = []\n",
        "    for thr in np.arange(0, 1, 0.01):\n",
        "        y_pred_thr[y_pred >= thr] = 1\n",
        "        y_pred_thr[y_pred < thr] = 0\n",
        "        cur_acc = classification_report(y_test, y_pred_thr, output_dict=True)['accuracy']\n",
        "        accs.append(cur_acc)\n",
        "        thrs.append(thr)\n",
        "    accs = torch.FloatTensor(accs)\n",
        "    thrs = torch.FloatTensor(thrs)\n",
        "    idx = accs.argmax()\n",
        "    best_thr = thrs[idx].item()\n",
        "    best_acc = accs[idx].item()\n",
        "    y_pred[y_pred >= best_thr] = 1\n",
        "    y_pred[y_pred < best_thr] = 0\n",
        "    return y_pred"
      ],
      "metadata": {
        "id": "A_tJWrPsizIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test data report"
      ],
      "metadata": {
        "id": "sAypngREspaF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "y_pred = get_binary_pred(y_test, y_pred.detach())\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "K6M8kB3Ni6-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Points\n",
        "\n",
        "points = model accuracy * 100\n",
        "\n",
        "model accuracy is the cell corresponding to accuracy and f1-score in the above report"
      ],
      "metadata": {
        "id": "a0zkvEgLtjeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Wf9euRyGzYzB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}